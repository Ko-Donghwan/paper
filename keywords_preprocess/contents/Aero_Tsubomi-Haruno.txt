<div class="model-card-content prose md:px-6 md:-mx-6 lg:-mr-20 lg:pr-20 xl:-mr-24 xl:pr-24 2xl:-mr-36 2xl:pr-36 hf-sanitized hf-sanitized-iBO_p1lBM-wjOIGo1chhM">
<!-- HTML_TAG_START --><h1 class="relative group flex items-center">
<a class="block pr-1.5 text-lg md:absolute md:p-1.5 md:opacity-0 md:group-hover:opacity-100 md:right-full" href="#dialogpt-trained-on-the-speech-of-a-game-character" id="dialogpt-trained-on-the-speech-of-a-game-character" rel="nofollow">
<span class="header-link"><svg aria-hidden="true" class="text-gray-500 hover:text-black dark:hover:text-gray-200 w-4" height="1em" preserveaspectratio="xMidYMid meet" role="img" viewbox="0 0 256 256" width="1em" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span>
</a>
<span>
		DialoGPT Trained on the Speech of a Game Character
	</span>
</h1>
<pre><code class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelWithLMHead
  
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"r3dhummingbird/DialoGPT-medium-joshua"</span>)
model = AutoModelWithLMHead.from_pretrained(<span class="hljs-string">"r3dhummingbird/DialoGPT-medium-joshua"</span>)
<span class="hljs-comment"># Let's chat for 4 lines</span>
<span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):
    <span class="hljs-comment"># encode the new user input, add the eos_token and return a tensor in Pytorch</span>
    new_user_input_ids = tokenizer.encode(<span class="hljs-built_in">input</span>(<span class="hljs-string">"&gt;&gt; User:"</span>) + tokenizer.eos_token, return_tensors=<span class="hljs-string">'pt'</span>)
    <span class="hljs-comment"># print(new_user_input_ids)</span>
    <span class="hljs-comment"># append the new user input tokens to the chat history</span>
    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-<span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> step &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> new_user_input_ids
    <span class="hljs-comment"># generated a response while limiting the total chat history to 1000 tokens, </span>
    chat_history_ids = model.generate(
        bot_input_ids, max_length=<span class="hljs-number">200</span>,
        pad_token_id=tokenizer.eos_token_id,  
        no_repeat_ngram_size=<span class="hljs-number">3</span>,       
        do_sample=<span class="hljs-literal">True</span>, 
        top_k=<span class="hljs-number">100</span>, 
        top_p=<span class="hljs-number">0.7</span>,
        temperature=<span class="hljs-number">0.8</span>
    )
    
    <span class="hljs-comment"># pretty print last ouput tokens from bot</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Tsubomi: {}"</span>.<span class="hljs-built_in">format</span>(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-<span class="hljs-number">1</span>]:][<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)))
</code></pre>
<!-- HTML_TAG_END --></div>