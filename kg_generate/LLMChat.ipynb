{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama # type: ignore\n",
    "from langchain_core.output_parsers import StrOutputParser # type: ignore\n",
    "from langchain_core.prompts import ChatPromptTemplate # type: ignore\n",
    "from langchain_core.output_parsers import JsonOutputParser # type: ignore\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # type: ignore\n",
    "from langchain_teddynote.messages import stream_response  # type: ignore\n",
    "from bs4 import BeautifulSoup # type: ignore\n",
    "from openai import OpenAI # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "from groq import Groq # type: ignore\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import requests # type: ignore\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = Groq(\n",
    "    api_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./models/{model_name}/Contents.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description = soup.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_item = {\n",
    "    \"ModelArchitecture\": \"The structure or framework of the model (e.g., Transformer, RNN, CNN)\",\n",
    "    \"Purpose\": \"The main use cases or applications the model is designed for (e.g., natural language processing, image recognition)\",\n",
    "    \"ModelTrainingTime\": \"The amount of time taken to train the model\",\n",
    "    \"MaintainerContact\": \"Contact information for the model's maintainers\",\n",
    "    \"RelatedDocumentation\": \"Documents or resources related to the model\",\n",
    "    \"CommunityFeedback\": \"Feedback from users or the community\",\n",
    "    \"InferenceSpeed\": \"The speed of the model's inference (how quickly it makes predictions)\",\n",
    "    \"PerformanceMetric\": \"Metrics used to evaluate the model’s performance (e.g., accuracy, F1 score)\",\n",
    "    \"KnownLimitations\": \"Known limitations or constraints of the model\",\n",
    "    \"IntendedUse\": \"The intended use or purpose of the model\",\n",
    "    \"EthicalConsiderations\": \"Ethical considerations when using the model\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1st = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Follow the instructions below to ensure that the Llama 3.1 model can accurately understand and perform the task.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Metadata Item:  \n",
    "{metadata_item}\n",
    "\n",
    "Model Description:  \n",
    "{model_description}\n",
    "\n",
    "Instructions:  \n",
    "1. Identify the paragraph(s) from the {model_name} model description that contain content related to the Metadata Item.  \n",
    "2. Extract and reproduce the entire paragraph(s) exactly as they appear in the original model_description.  \n",
    "3. Do not make any changes, interpretations, or omissions. Preserve the original context and meaning fully.  \n",
    "4. Extract the paragraph(s) as generously as possible, even if the content may only be indirectly related to the Metadata Item.  \n",
    "5. Double-check that all paragraphs relevant to the Metadata Item have been included.  \n",
    "6. Do not generate or add any additional information beyond what is explicitly present in the model description.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer:\n",
    "<|eot_id|>                                                                                                           \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_chat = groq_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_1st\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_related_metadataitem = groq_chat.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2nd = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Please carefully follow the instructions to help the Llama 3 model understand and complete the task accurately.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Metadata Item:  \n",
    "{metadata_item}\n",
    "\n",
    "Model Description related to Metadata Item:  \n",
    "{content_related_metadataitem}\n",
    "\n",
    "Instructions:  \n",
    "1. Find and extract the specific instances from the {model_name} model description that directly relate to the Metadata Item.  \n",
    "2. Reproduce the instance(s) exactly as they appear without making any modifications or omissions.\n",
    "3. Do not add any additional information beyond what is present in the original description.\n",
    "4. Do not include long sentences, descriptions, or interpretations. Only extract and return the value or keyword that directly corresponds to the metadata item.\n",
    "5. Ensure all matched metadata items and their corresponding values are returned in valid JSON format. Follow strict JSON formatting rules.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer:\n",
    "<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_chat = groq_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_2nd\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = groq_chat.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"\"\"{res}\\n\\n\n",
    "                Above content provided contains the description for the {model_name} model. \n",
    "                Please extract instances that correspond to the following metadata items.\n",
    "                *** \n",
    "                If a specific item is not mentioned in the content, insert a null value for that metadata.\n",
    "                Match the metadata items with specific values or keywords found in the model description.\n",
    "                Avoid long descriptive sentences or explanations—only extract the direct value or keyword that matches the metadata item.\n",
    "                ***\n",
    "                Metadata items to map:\\n{metadata_item}\\n\n",
    "                Once extracted, please return the result in a well-formatted JSON structure. \n",
    "                Ensure the output adheres to proper JSON syntax. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_chat = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        }           \n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = openai_chat.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the extracted metadata in valid JSON format based on the provided content:\\n\\n```json\\n{\\n  \"ModelArchitecture\": \"transformers\",\\n  \"Purpose\": \"sequence classification, token classification or question answering\",\\n  \"ModelTrainingTime\": \"one million steps\",\\n  \"MaintainerContact\": null,\\n  \"RelatedDocumentation\": null,\\n  \"CommunityFeedback\": null,\\n  \"InferenceSpeed\": null,\\n  \"PerformanceMetric\": \"Glue test results: MNLI-(m/mm), QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE\",\\n  \"KnownLimitations\": \"biased predictions\",\\n  \"IntendedUse\": \"masked language modeling, next sentence prediction, fine-tuning on downstream tasks\",\\n  \"EthicalConsiderations\": \"biased predictions\"\\n}\\n```'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_inst = re.search(r'(\\{.*\\})', response_message, re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"ModelArchitecture\": \"transformers\",\\n  \"Purpose\": \"sequence classification, token classification or question answering\",\\n  \"ModelTrainingTime\": \"one million steps\",\\n  \"MaintainerContact\": null,\\n  \"RelatedDocumentation\": null,\\n  \"CommunityFeedback\": null,\\n  \"InferenceSpeed\": null,\\n  \"PerformanceMetric\": \"Glue test results: MNLI-(m/mm), QQP, QNLI, SST-2, CoLA, STS-B, MRPC, RTE\",\\n  \"KnownLimitations\": \"biased predictions\",\\n  \"IntendedUse\": \"masked language modeling, next sentence prediction, fine-tuning on downstream tasks\",\\n  \"EthicalConsiderations\": \"biased predictions\"\\n}'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_inst.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_inst = gpt_inst.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_inst = json.loads(chatgpt_inst) # 문자열을 딕셔너리로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-cased'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f'./models/{model_name}/'\n",
    "\n",
    "with open(folder_path + 'Contents.json', 'w') as f:\n",
    "    json.dump(chatgpt_inst, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
